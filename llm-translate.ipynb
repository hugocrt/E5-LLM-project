{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 10755955,
     "sourceType": "datasetVersion",
     "datasetId": 6671320
    },
    {
     "sourceId": 10758442,
     "sourceType": "datasetVersion",
     "datasetId": 6673220
    }
   ],
   "dockerImageVersionId": 30887,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "c4f47146ccd79ff3",
   "cell_type": "markdown",
   "source": [
    "# Projet Final de LLM - CARANGEOT Hugo, CONTE-DEVOLX Titouan - E5DSIA - 2024/2025 - ESIEE Paris"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "ce6fc8fee8283eb4",
   "cell_type": "markdown",
   "source": [
    "Dans un premier temps, rappelons rapidement le projet encadré par Monsieur Badr TAJINI, dans le cadre de l'unité LLM en 5ᵉ année d'ESIEE Paris.\n",
    "\n",
    "\"\n",
    "**Project Summary**\n",
    "This final project focuses on creating a basic AI assistant capable of following everyday instructions, built on\n",
    "a GPT-2 model that is instruction-fine-tuned. The goal is to practice transforming a pretrained LLM (like\n",
    "GPT-2) into a system that can respond effectively to various daily queries and tasks (e.g., scheduling\n",
    "reminders, giving recipe suggestions, generating short emails).\n",
    "You will leverage their understanding of pretraining, fine-tuning, and, most importantly, instruction-based\n",
    "fine-tuning developed across the course (Chapters 5, 6, and 7). The project also draws on the methodology\n",
    "seen in Labs 6-7, where a dataset was prepared for supervised instruction fine-tuning using an Alpaca-style\n",
    "prompt format. By the end of this project, you will have a working prototype of a GPT-2–based assistant,\n",
    "demonstrating how instruction fine-tuning can dramatically improve the model’s ability to follow user\n",
    "commands and prompts\n",
    "\" *(c.f. final-project-llms-2025.md, Teams)*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "eee7ce0ed49b5589",
   "cell_type": "markdown",
   "source": [
    "Nous allons suivre étape par étape les consignes pour ce notebook, à savoir : \n",
    "\n",
    "1) *'Environment Setup & Base Model Initialization'*\n",
    "2) *'Instruction Dataset Preparation'*\n",
    "3) *'Instruction Fine-Tuning'*\n",
    "4) *'Evaluation & Iterative Improvement'*\n",
    "5) *'Deployment & Final Presentation'*\n",
    "\n",
    "L'objectif final étant d'avoir un assistant personnel et de le démontrer.\n",
    "\n",
    "Enfin, nous allons utiliser des fonctions tirées du cours afin d'accélérer le temps de développement du projet."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "c6ec3a16d3947045",
   "cell_type": "markdown",
   "source": [
    "## 1 - *'Environment Setup & Base Model Initialization'*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "23372f8337d03d9c",
   "cell_type": "markdown",
   "source": [
    "#### Important !\n",
    "\n",
    "On ne dispose que d'une carte **NVIDIA GTX 1660ti 6Go**, cela n'était clairement pas suffisant pour entrainer notre modèle (que l'on verra par la suite) puisque nous avions des **OOM** [Out Of Memory error] malgré le réglage des paramètres. On a donc fait le tour de différents **services cloud** pour entrainer notre modèle avec des **GPU** plus **performants**. La meilleure offre que l'on a trouvée est celle de **<a href=https://www.kaggle.com>Kaggle</a>**. Nous disposons de **30h** par **semaine** de **GPU** **gratuitement** (avec 12h de session maximum). Ainsi pour entrainer notre modèle, nous avons utilisé en **parallèle deux GPU tesla T4** (on avait au choix : GPU T4 x 2; GPU P100; TPU VM v3-8) ce qui nous a dans un premier temps, tout simplement permis d'entrainer notre modèle qui est un peu lourd et en plus de le faire rapidement grâce à l'utilisation parallèle des deux GPU T4 offerts par Kaggle.\n",
    "\n",
    "Pour utiliser Kaggle, nous devons importer nos fichiers locaux sur l'interface Web (nous avons déjà créé un dataset contenant nos fichiers python...) et les rendre utilisables en les mettant dans le dossier **working**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "ba35a94c-40aa-42b3-85bd-b27f8664d419",
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Créer le dossier cible s'il n'existe pas\n",
    "os.makedirs(\"/kaggle/working/projet\", exist_ok=True)\n",
    "\n",
    "# Copier tous les fichiers depuis /kaggle/input/vers /kaggle/working/projet/\n",
    "shutil.copytree(\"/kaggle/input/librairies-hugo-llm-final-project\", \"/kaggle/working/project\", dirs_exist_ok=True)\n",
    "\n",
    "print(\"Fichiers et dossiers copiés dans /kaggle/working/project/\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T13:58:50.875109Z",
     "iopub.execute_input": "2025-02-17T13:58:50.875394Z",
     "iopub.status.idle": "2025-02-17T13:58:50.906322Z",
     "shell.execute_reply.started": "2025-02-17T13:58:50.875361Z",
     "shell.execute_reply": "2025-02-17T13:58:50.905474Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-15T13:59:57.324784700Z",
     "start_time": "2025-02-15T13:59:57.204905600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Fichiers et dossiers copiés dans /kaggle/working/project/\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "id": "081d0ecc-c120-4bd5-bace-fe561a0f35fd",
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Lister les fichiers dans le répertoire de travail actuel\n",
    "print(os.listdir(\"/kaggle/working/project\"))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T13:58:50.907108Z",
     "iopub.execute_input": "2025-02-17T13:58:50.907308Z",
     "iopub.status.idle": "2025-02-17T13:58:50.911994Z",
     "shell.execute_reply.started": "2025-02-17T13:58:50.907290Z",
     "shell.execute_reply": "2025-02-17T13:58:50.911079Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-15T13:59:57.328823700Z",
     "start_time": "2025-02-15T13:59:57.319089300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['gpt_instruction_finetuning.py', 'previous_labs.py', 'gpt_download.py']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "id": "6036622f-e0a8-48d9-baf2-11427ef76fc9",
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/project\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T13:58:50.913134Z",
     "iopub.execute_input": "2025-02-17T13:58:50.913441Z",
     "iopub.status.idle": "2025-02-17T13:58:50.923562Z",
     "shell.execute_reply.started": "2025-02-17T13:58:50.913395Z",
     "shell.execute_reply": "2025-02-17T13:58:50.922719Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-15T13:59:57.443310400Z",
     "start_time": "2025-02-15T13:59:57.330821200Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "id": "b15cc3eda643c8cf",
   "cell_type": "markdown",
   "source": [
    "Maintenant que nous avons importé nos différents fichiers, commençons le projet.\n",
    "Importons notre modèle GPT-2. Nous utiliserons la version medium (355M param) pour un meilleur rendu final même si plus lourd au fine-tuning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "529d9a322847511b",
   "cell_type": "code",
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_labs import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T13:58:50.925741Z",
     "iopub.execute_input": "2025-02-17T13:58:50.926041Z",
     "iopub.status.idle": "2025-02-17T14:00:52.655491Z",
     "shell.execute_reply.started": "2025-02-17T13:58:50.926012Z",
     "shell.execute_reply": "2025-02-17T14:00:52.654536Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T22:42:35.291413300Z",
     "start_time": "2025-02-17T22:42:24.021487700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "id": "236615bf9dc4e4a3",
   "cell_type": "markdown",
   "source": [
    "Vérifions ensuite que notre modèle est bien importé et fonctionnel en lui faisant générer du texte, la phrase générée doit avoir du sens pour affirmer que notre modèle est bien instancié."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "595c96cbf49f7ad7",
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:00:52.656720Z",
     "iopub.execute_input": "2025-02-17T14:00:52.657216Z",
     "iopub.status.idle": "2025-02-17T14:00:53.871535Z",
     "shell.execute_reply.started": "2025-02-17T14:00:52.657192Z",
     "shell.execute_reply": "2025-02-17T14:00:53.870818Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T22:42:38.237572200Z",
     "start_time": "2025-02-17T22:42:38.220788800Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "id": "4015d7d684021043",
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from previous_labs import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Translate in french: Hello World !\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:00:53.872333Z",
     "iopub.execute_input": "2025-02-17T14:00:53.872667Z",
     "iopub.status.idle": "2025-02-17T14:01:00.277759Z",
     "shell.execute_reply.started": "2025-02-17T14:00:53.872635Z",
     "shell.execute_reply": "2025-02-17T14:01:00.276776Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T22:42:48.077641400Z",
     "start_time": "2025-02-17T22:42:39.906547600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Translate in french: Hello World !\n",
      "\n",
      "This is a quick tutorial on translating a simple web application into another native language.\n",
      "\n",
      "You can find the source\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "id": "c23b08b0712d8e0f",
   "cell_type": "markdown",
   "source": [
    "Le texte généré a du sens même s'il ne répond pas au problème (c'est pour cela qu'on va le fine-tuner), ce qui prouve que notre modèle a été correctement chargé."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "12b8f687a2162080",
   "cell_type": "markdown",
   "source": [
    "Enfin, pour réduire radicalement le temps \"d'entrainement\" (ici, c'est plus du fine-tuning) de notre modèle, nous utiliserons les deux GPU T4 et les ferons tourner parallèlement. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "c0c05f9b116f2bd5",
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Utilisation de {torch.cuda.device_count()} GPUs\")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:00.278737Z",
     "iopub.execute_input": "2025-02-17T14:01:00.279037Z",
     "iopub.status.idle": "2025-02-17T14:01:00.997590Z",
     "shell.execute_reply.started": "2025-02-17T14:01:00.279014Z",
     "shell.execute_reply": "2025-02-17T14:01:00.996875Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T09:33:32.581556Z",
     "start_time": "2025-02-17T09:33:31.962661400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Utilisation de 2 GPUs\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "id": "9b309af3f2ac8f77",
   "cell_type": "markdown",
   "source": [
    "## 2 - *'Instruction Dataset Preparation'*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "e875accef6cc645a",
   "cell_type": "markdown",
   "source": [
    "Nous avons bien compris qu'il est **compliqué** de **créer un assistant** très **généraliste performant**, surtout pour les petits modèles que nous entraînons. Il est donc préférable de concevoir plusieurs **assistants spécialisés** dans des tâches spécifiques. Ici, nous nous concentrerons sur la création d’un **assistant dédié à la traduction de l'anglais au français**.\n",
    "\n",
    "Pour cela, nous utiliserons le dataset **<a href=https://huggingface.co/datasets/FrancophonIA/English-French>FrancophonIA/English-French</a>**.\n",
    "La **longueur maximale** des textes en **anglais** est de **262** caractères, tandis que celle des textes en **français** est de **325** caractères. Cependant, la longueur en tokens peut différer (et c'est même presque toujours le cas), car un caractère ne correspond pas systématiquement à un token.\n",
    "\n",
    "### Gestion des tokens\n",
    "\n",
    "D'après **OpenAI** (c.f. https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them):\n",
    "\n",
    "    \"1,500 words ~= 2048 tokens\"\n",
    "\n",
    "Ce qui donne une approximation de **1024 tokens ≈ 750 mots**.\n",
    "\n",
    "Notre dataset suit le format **Parallel Corpus** (**Anglais** ↔ **Français** pour chaque ligne), ce qui est idéal pour fine-tuner notre modèle. Cependant, **GPT-2** est pré-entraîné sur du **texte brut**, donc nous devons concaténer les entrées et sorties en une seule séquence pour respecter ce format.\n",
    "\n",
    "### Longueurs des mots et tokenization\n",
    "\n",
    "Après une courte recherche sur Google, nous trouvons:\n",
    "\n",
    "- **Anglais** : ~**4.7** caractères/mot\n",
    "- **Français** : ~**5.1** caractères/mot\n",
    "\n",
    "Si l'on considère une moyenne de 5 caractères par mot, alors une phrase combinant un texte en anglais (262 caractères) et un texte en français (325 caractères) représente environ :\n",
    "\n",
    "(262 + 325)/5 ~= 118 mots ~ 161 tokens \n",
    "\n",
    "### Dépassement du max_length de 1024 tokens\n",
    "\n",
    "Nous sommes donc largement **sous** la **limite** des **1024** tokens. Cela signifie que nous pouvons ajouter des consignes via du **prompt engineering** pour **améliorer** l'**entraînement** du modèle sans dépasser cette contrainte.\n",
    "\n",
    "Si certaines réponses venaient à dépasser cette limite, nous appliquerons une **troncature**, bien que ce cas soit peu probable, car le modèle **BPE** utilisé par **GPT-2** segmente efficacement les mots fréquents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comme nous avons pris un dataset sur **HuggingFace**, il existe une libraire python 'datasets' pour l'importer directement."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "232c16a08a55ff25"
  },
  {
   "id": "bdffda8ff958f51",
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "import tqdm\n",
    "\n",
    "dataset = datasets.load_dataset(\"FrancophonIA/English-French\")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:00.998227Z",
     "iopub.execute_input": "2025-02-17T14:01:00.998456Z",
     "iopub.status.idle": "2025-02-17T14:01:03.904068Z",
     "shell.execute_reply.started": "2025-02-17T14:01:00.998425Z",
     "shell.execute_reply": "2025-02-17T14:01:03.903381Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T13:01:40.156436500Z",
     "start_time": "2025-02-18T13:01:38.511413400Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "id": "8ee05d2eee41f8cd",
   "cell_type": "code",
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:03.904838Z",
     "iopub.execute_input": "2025-02-17T14:01:03.905530Z",
     "iopub.status.idle": "2025-02-17T14:01:03.909760Z",
     "shell.execute_reply.started": "2025-02-17T14:01:03.905494Z",
     "shell.execute_reply": "2025-02-17T14:01:03.908922Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T13:01:41.511685700Z",
     "start_time": "2025-02-18T13:01:41.486654900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English words/sentences', 'French words/sentences'],\n",
      "        num_rows: 175621\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "id": "b0de708fc550d557",
   "cell_type": "code",
   "source": [
    "print(dataset[\"train\"][0])  "
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:03.910489Z",
     "iopub.execute_input": "2025-02-17T14:01:03.910706Z",
     "iopub.status.idle": "2025-02-17T14:01:03.929694Z",
     "shell.execute_reply.started": "2025-02-17T14:01:03.910684Z",
     "shell.execute_reply": "2025-02-17T14:01:03.928880Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T13:01:43.702680100Z",
     "start_time": "2025-02-18T13:01:43.666564900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English words/sentences': 'Hi.', 'French words/sentences': 'Salut!'}\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "id": "80561a6b455e744c",
   "cell_type": "markdown",
   "source": [
    "Nous avons fait un tour rapidement de quelques lignes pour s'assurer qu'elles étaient **bien rédigées**, etc. Il y a **175,621 lignes** ce qui permet une **traduction** assez **complète**. De plus, \"author only used sentences that were owned by identified native speakers working on the Tatoeba Project and English sentences that he personally checked and did not reject.\" (c.f. https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench).\n",
    "Nous avons donc un **indice de confiance élevé** pour ce **dataset**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "49116b7fdf3b7761",
   "cell_type": "markdown",
   "source": [
    "Nous allons également séparer nos données en sets train, test, validation puisqu'ils n'existent pas de base."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "7551bb43704b8716",
   "cell_type": "code",
   "source": [
    "data = dataset['train'].shuffle(seed=123)\n",
    "\n",
    "# On définit nos portions\n",
    "train_portion = int(len(data) * 0.80)\n",
    "val_portion = int(len(data) * 0.10)\n",
    "\n",
    "# Séparation en train (80%), validation (10%) et test (10%)\n",
    "train_data = data.select(range(train_portion))\n",
    "val_data = data.select(range(train_portion, train_portion + val_portion))\n",
    "test_data = data.select(range(train_portion + val_portion, len(data)))"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:03.930581Z",
     "iopub.execute_input": "2025-02-17T14:01:03.930848Z",
     "iopub.status.idle": "2025-02-17T14:01:03.991061Z",
     "shell.execute_reply.started": "2025-02-17T14:01:03.930827Z",
     "shell.execute_reply": "2025-02-17T14:01:03.990462Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T13:01:50.650713800Z",
     "start_time": "2025-02-18T13:01:50.619712700Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "id": "5841adf6584a47dd",
   "cell_type": "code",
   "source": [
    "print(len(train_data), len(val_data), len(test_data))\n",
    "print(train_data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:03.991815Z",
     "iopub.execute_input": "2025-02-17T14:01:03.992017Z",
     "iopub.status.idle": "2025-02-17T14:01:03.997263Z",
     "shell.execute_reply.started": "2025-02-17T14:01:03.991999Z",
     "shell.execute_reply": "2025-02-17T14:01:03.996463Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T13:01:52.466964900Z",
     "start_time": "2025-02-18T13:01:52.443828900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140496 17562 17563\n",
      "{'English words/sentences': \"I'll try to do better next time.\", 'French words/sentences': \"J'essayerai de faire mieux la prochaine fois.\"}\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "id": "6576a53b6d64cd4",
   "cell_type": "markdown",
   "source": [
    "On voit bien la répartition des différents ensembles."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "be55f5db69dd264a",
   "cell_type": "markdown",
   "source": [
    "Concaténons les questions et réponses en une seule phrase pour obtenir notre texte brut pour le fine-tuning. Appliquons également du **Prompt Engineering** pour aligner notre modèle sur nos attentes et optimiser ses performances.  \n",
    "\n",
    "Pour cela, définissons :  \n",
    "- **Un rôle** pour guider son comportement,  \n",
    "- **Une tâche** pour préciser l’objectif,  \n",
    "- Enfin, **la question et la réponse**, afin qu’il apprenne le format attendu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "1a67da735dc82b3f",
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_texts = []\n",
    "\n",
    "        # Tokenisation des données avec un rôle explicite\n",
    "        for entry in data:\n",
    "            instruction = (\n",
    "                \"\\n\\n### Role: You are a translation assistant.\"\n",
    "                \"\\n### Task: Translate the given English sentence into French.\"\n",
    "                f\"\\n### English Sentence:\\n{entry['English words/sentences']}\"\n",
    "            )\n",
    "            response = f\"\\n\\n### French Translation:\\n{entry['French words/sentences']} <|endoftext|>\"\n",
    "\n",
    "            full_text = instruction + response\n",
    "            self.encoded_texts.append(\n",
    "                self.tokenizer.encode(full_text, allowed_special={\"<|endoftext|>\"})\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:03.999760Z",
     "iopub.execute_input": "2025-02-17T14:01:03.999961Z",
     "iopub.status.idle": "2025-02-17T14:01:04.013641Z",
     "shell.execute_reply.started": "2025-02-17T14:01:03.999943Z",
     "shell.execute_reply": "2025-02-17T14:01:04.012843Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-15T16:33:13.757604800Z",
     "start_time": "2025-02-15T16:33:13.749936500Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "id": "48c28ce5faf4d886",
   "cell_type": "markdown",
   "source": [
    "Par ailleurs, créons des batchs de taille dynamique (plus grande entrée ou 1024 max) afin d'améliorer notre entraînement."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "b067d26166492329",
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "from gpt_instruction_finetuning import custom_collate_fn\n",
    "\n",
    "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-15T16:33:26.006995900Z",
     "start_time": "2025-02-15T16:33:25.994350100Z"
    },
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:04.014958Z",
     "iopub.execute_input": "2025-02-17T14:01:04.015172Z",
     "iopub.status.idle": "2025-02-17T14:01:04.027684Z",
     "shell.execute_reply.started": "2025-02-17T14:01:04.015146Z",
     "shell.execute_reply": "2025-02-17T14:01:04.026768Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "id": "3f14edfeb622a95e",
   "cell_type": "markdown",
   "source": [
    "Maintenant, il ne reste plus qu'à en faire des data loaders."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "85750966698b3368",
   "cell_type": "code",
   "source": [
    "num_workers = 0 # CPU trop faible pour +\n",
    "batch_size = 16  # bas pour éviter OOM\n",
    "\n",
    "train_dataset = TranslationDataset(train_data, tokenizer=tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = TranslationDataset(val_data, tokenizer=tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = TranslationDataset(test_data, tokenizer=tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:04.028532Z",
     "iopub.execute_input": "2025-02-17T14:01:04.028822Z",
     "iopub.status.idle": "2025-02-17T14:01:22.973579Z",
     "shell.execute_reply.started": "2025-02-17T14:01:04.028793Z",
     "shell.execute_reply": "2025-02-17T14:01:22.972623Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-15T16:33:31.187777600Z",
     "start_time": "2025-02-15T16:33:30.103442600Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "id": "34ae5d088eebd586",
   "cell_type": "markdown",
   "source": [
    "## 3 - *'Instruction Fine-Tuning'*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "355b0517d0eacacc",
   "cell_type": "markdown",
   "source": [
    "Il est maintenant temps de lancer \"l'apprentissage\" de notre modèle !\n",
    "\n",
    "Le learning rate est de 5e-5, ce qui est souvent utilisé pour le fine-tuning d'un modèle pré-entrainé comme GPT-2.\n",
    "\n",
    "Le batch_size est de 16, nous avons commencé à 32, mais ne disposions pas d'assez de mémoire (OOM).\n",
    "\n",
    "Nous avons également mis 2 epochs, c'est une valeur souvent utilisée pour fine-tuner en évitant de trop sur-apprendre et pour ne pas entrainer pendant trop longtemps (l'entrainement a pris presque 6h sur Kaggle, il aurait fallu près de 54 jours sur notre ordinateur)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "7172669173ee1c19",
   "cell_type": "code",
   "source": [
    "from previous_labs import train_model_simple\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "num_epochs = 2\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "model.to(device)\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader, \n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=500,\n",
    "    eval_iter=100,\n",
    "    start_context=val_data[0][\"English words/sentences\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T14:01:22.974531Z",
     "iopub.execute_input": "2025-02-17T14:01:22.974851Z",
     "iopub.status.idle": "2025-02-17T19:48:59.426098Z",
     "shell.execute_reply.started": "2025-02-17T14:01:22.974819Z",
     "shell.execute_reply": "2025-02-17T19:48:59.425183Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-15T17:05:13.156829400Z",
     "start_time": "2025-02-15T17:05:02.577949200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Ep 1 (Step 000000): Train loss 2.546, Val loss 2.550\nEp 1 (Step 000500): Train loss 0.737, Val loss 0.745\nEp 1 (Step 001000): Train loss 0.634, Val loss 0.659\nEp 1 (Step 001500): Train loss 0.611, Val loss 0.632\nEp 1 (Step 002000): Train loss 0.572, Val loss 0.585\nEp 1 (Step 002500): Train loss 0.531, Val loss 0.570\nEp 1 (Step 003000): Train loss 0.524, Val loss 0.552\nEp 1 (Step 003500): Train loss 0.507, Val loss 0.529\nEp 1 (Step 004000): Train loss 0.479, Val loss 0.524\nEp 1 (Step 004500): Train loss 0.474, Val loss 0.509\nEp 1 (Step 005000): Train loss 0.465, Val loss 0.507\nEp 1 (Step 005500): Train loss 0.457, Val loss 0.497\nEp 1 (Step 006000): Train loss 0.452, Val loss 0.494\nEp 1 (Step 006500): Train loss 0.445, Val loss 0.494\nEp 1 (Step 007000): Train loss 0.431, Val loss 0.482\nEp 1 (Step 007500): Train loss 0.418, Val loss 0.487\nEp 1 (Step 008000): Train loss 0.408, Val loss 0.482\nEp 1 (Step 008500): Train loss 0.406, Val loss 0.479\nShe showed him the photo.  I'm not sure what to do with it.  I'm not sure what to do with it.  I'm not sure what to do with it.  I'm not sure what to do with it.  \nEp 2 (Step 009000): Train loss 0.398, Val loss 0.469\nEp 2 (Step 009500): Train loss 0.392, Val loss 0.479\nEp 2 (Step 010000): Train loss 0.389, Val loss 0.470\nEp 2 (Step 010500): Train loss 0.389, Val loss 0.471\nEp 2 (Step 011000): Train loss 0.383, Val loss 0.466\nEp 2 (Step 011500): Train loss 0.369, Val loss 0.461\nEp 2 (Step 012000): Train loss 0.372, Val loss 0.454\nEp 2 (Step 012500): Train loss 0.367, Val loss 0.452\nEp 2 (Step 013000): Train loss 0.361, Val loss 0.463\nEp 2 (Step 013500): Train loss 0.361, Val loss 0.457\nEp 2 (Step 014000): Train loss 0.352, Val loss 0.445\nEp 2 (Step 014500): Train loss 0.356, Val loss 0.462\nEp 2 (Step 015000): Train loss 0.346, Val loss 0.452\nEp 2 (Step 015500): Train loss 0.344, Val loss 0.442\nEp 2 (Step 016000): Train loss 0.341, Val loss 0.440\nEp 2 (Step 016500): Train loss 0.334, Val loss 0.441\nEp 2 (Step 017000): Train loss 0.330, Val loss 0.448\nEp 2 (Step 017500): Train loss 0.325, Val loss 0.445\nShe showed him the photo.  ### Role: You are a translation assistant. ### Task: Translate the photo into French. ### English Sentence: I'm not sure that's a good idea.  ### French Translation: Je ne suis\nTraining completed in 347.61 minutes.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "id": "7c8cf8f1186095dc",
   "cell_type": "markdown",
   "source": [
    "On enregistre notre modèle pour le réutiliser par la suite."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "ff7719478b46cba3",
   "cell_type": "code",
   "source": [
    "torch.save(model, 'ft-model.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T19:48:59.427034Z",
     "iopub.execute_input": "2025-02-17T19:48:59.427709Z",
     "iopub.status.idle": "2025-02-17T19:49:01.896655Z",
     "shell.execute_reply.started": "2025-02-17T19:48:59.427685Z",
     "shell.execute_reply": "2025-02-17T19:49:01.895869Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-15T14:00:06.162820700Z",
     "start_time": "2025-02-15T14:00:06.157785400Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "id": "bac13ae1-fbe1-4842-a571-05408a06d1ec",
   "cell_type": "code",
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "display(FileLink(\"ft-model.pth\"))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T19:49:01.897388Z",
     "iopub.execute_input": "2025-02-17T19:49:01.897624Z",
     "iopub.status.idle": "2025-02-17T19:49:01.902533Z",
     "shell.execute_reply.started": "2025-02-17T19:49:01.897605Z",
     "shell.execute_reply": "2025-02-17T19:49:01.901799Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "/kaggle/working/ft-model.pth",
      "text/html": "<a href='ft-model.pth' target='_blank'>ft-model.pth</a><br>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 18
  },
  {
   "id": "7caf2e608e8e7b0c",
   "cell_type": "code",
   "source": [
    "from previous_labs import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T19:49:01.903147Z",
     "iopub.execute_input": "2025-02-17T19:49:01.903332Z",
     "iopub.status.idle": "2025-02-17T19:49:03.085826Z",
     "shell.execute_reply.started": "2025-02-17T19:49:01.903315Z",
     "shell.execute_reply": "2025-02-17T19:49:03.084950Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 500x300 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "execution_count": 19
  },
  {
   "id": "c86f099354669d6",
   "cell_type": "markdown",
   "source": [
    "On s'aperçoit que notre modèle **apprend très vite** au **début** puisqu'il passe d'un/e training/val loss de 2.5 à ~0.7 au tout début de la première epoch. Par la suite, nous remarquons que nos deux loss continuent de baisser jusqu'au milieu de l'epoch 1 à 2. Cependant, nous remarquons que sur la fin, notre training loss continue de descendre et notre validation loss stagne, cela reflète un **over fitting** (assez léger dans notre cas).\n",
    "\n",
    "Nous pensons donc avoir mis des bons paramètres d'entrainement pour notre modèle.\n",
    "Vérifions cela dans la prochaine partie."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "9cdefc76ea25a18a",
   "cell_type": "markdown",
   "source": [
    "## 4 - *'Evaluation & Iterative Improvement'*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "633f35b8b03215f6",
   "cell_type": "markdown",
   "source": [
    "Nous allons repasser en local (plus sur Kaggle) pour **évaluer** notre modèle grâce à la **perplexité** via **ollama -> llama3**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "2cd469cbd1c0cd",
   "cell_type": "markdown",
   "source": [
    "#### Ici, nous allons recharger notre modèle avec les param d'entrainement enregistrés pour évaluer le modèle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "id": "b5c7f5316aa25c2c",
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Charger le modèle sauvegardé (sur plusieurs GPUs)\n",
    "ft_model = torch.load('ft-model.pth', map_location=\"cuda\")  # Charger sur un seul GPU\n",
    "\n",
    "# Si le modèle est encapsulé dans DataParallel, on retire l'encapsulation\n",
    "if isinstance(ft_model, torch.nn.DataParallel):\n",
    "    ft_model = ft_model.module  # Récupérer le modèle original\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "ft_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T19:49:03.086844Z",
     "iopub.execute_input": "2025-02-17T19:49:03.087543Z",
     "iopub.status.idle": "2025-02-17T19:49:04.043137Z",
     "shell.execute_reply.started": "2025-02-17T19:49:03.087516Z",
     "shell.execute_reply": "2025-02-17T19:49:04.042381Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T08:38:23.862372600Z",
     "start_time": "2025-02-18T08:38:17.500371100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugoc\\AppData\\Local\\Temp\\ipykernel_21904\\410182289.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ft_model = torch.load('ft-model.pth', map_location=\"cuda\")  # Charger sur un seul GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": "GPTModel(\n  (tok_emb): Embedding(50257, 1024)\n  (pos_emb): Embedding(1024, 1024)\n  (drop_emb): Dropout(p=0.0, inplace=False)\n  (trf_blocks): Sequential(\n    (0): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (1): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (2): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (3): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (4): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (5): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (6): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (7): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (8): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (9): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (10): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (11): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (12): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (13): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (14): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (15): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (16): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (17): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (18): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (19): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (20): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (21): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (22): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n    (23): TransformerBlock(\n      (att): MultiHeadAttention(\n        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (ff): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=1024, out_features=4096, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n      )\n      (norm1): LayerNorm()\n      (norm2): LayerNorm()\n      (drop_resid): Dropout(p=0.0, inplace=False)\n    )\n  )\n  (final_norm): LayerNorm()\n  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "id": "c89cb2406b566428",
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T19:49:04.043870Z",
     "iopub.execute_input": "2025-02-17T19:49:04.044079Z",
     "iopub.status.idle": "2025-02-17T19:49:04.047390Z",
     "shell.execute_reply.started": "2025-02-17T19:49:04.044061Z",
     "shell.execute_reply": "2025-02-17T19:49:04.046804Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T08:38:28.776888800Z",
     "start_time": "2025-02-18T08:38:28.496654900Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons écrit un **code python** permettant d'utiliser notre modèle via un **CLI**. Nous avons également écrit une fonction pour **traduire** un texte **anglais** en **français** à l'aide de notre modèle. Vérifions que notre modèle fonctionne avant de calculer sa perplexité."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8856935552272590"
  },
  {
   "id": "f201a3a8b5e1ea00",
   "cell_type": "code",
   "source": [
    "from translate import translate\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "text_to_translate = \"i need a backpack\"\n",
    "\n",
    "translation = translate(text_to_translate, ft_model, tokenizer)\n",
    "print(f\"Traduction : {translation}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T19:49:04.048096Z",
     "iopub.execute_input": "2025-02-17T19:49:04.048276Z",
     "iopub.status.idle": "2025-02-17T19:49:22.672967Z",
     "shell.execute_reply.started": "2025-02-17T19:49:04.048260Z",
     "shell.execute_reply": "2025-02-17T19:49:22.672054Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T08:38:34.828321300Z",
     "start_time": "2025-02-18T08:38:33.075321400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduction : J'ai besoin d'un sac à dos.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Faisons** toutes les **traductions** par notre modèle pour le **set de test** et **enregistrons** tout cela dans un **json** afin de pouvoir calculer la **perplexité** avec **ollama**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20fff2546fe31c22"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English words/sentences': ['Tom hid behind a tree.', \"Don't they drive you mad?\"], 'French words/sentences': ['Tom se cacha derrière un arbre.', 'Ne vous font-elles pas tourner bourriques ?']}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T12:50:47.751816800Z",
     "start_time": "2025-02-18T12:50:47.597555100Z"
    }
   },
   "id": "73b3e64c24e82b09",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = [\n",
    "    {'English words/sentences': eng, 'French words/sentences': fr} \n",
    "    for eng, fr \n",
    "    in zip(test_data['English words/sentences'], test_data['French words/sentences'])\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T13:02:35.244983700Z",
     "start_time": "2025-02-18T13:02:34.794757Z"
    }
   },
   "id": "ca895b9de2beb33b",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'English words/sentences': 'Tom hid behind a tree.', 'French words/sentences': 'Tom se cacha derrière un arbre.'}, {'English words/sentences': \"Don't they drive you mad?\", 'French words/sentences': 'Ne vous font-elles pas tourner bourriques ?'}]\n",
      "17563\n"
     ]
    }
   ],
   "source": [
    "print(test[:2])\n",
    "print(test.__len__())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T13:02:36.975991400Z",
     "start_time": "2025-02-18T13:02:36.921354800Z"
    }
   },
   "id": "67c35cce124650d4",
   "execution_count": 44
  },
  {
   "id": "9cdb689b38bea5e3",
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "column_translated = []\n",
    "\n",
    "for i, entry in tqdm(enumerate(test), total=len(test)):\n",
    "    translation = translate(test[i]['English words/sentences'], ft_model, tokenizer)\n",
    "    test[i][\"model_response\"] = translation"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T19:49:22.673952Z",
     "iopub.execute_input": "2025-02-17T19:49:22.674282Z",
     "iopub.status.idle": "2025-02-17T19:49:22.726596Z",
     "shell.execute_reply.started": "2025-02-17T19:49:22.674247Z",
     "shell.execute_reply": "2025-02-17T19:49:22.725360Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T17:00:52.198886400Z",
     "start_time": "2025-02-18T13:02:55.353334800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17563/17563 [3:57:56<00:00,  1.23it/s]  \n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"translate.json\", \"w\", encoding='utf-8') as file:\n",
    "    json.dump(test, file, indent=4, ensure_ascii=False)  # \"indent\" for pretty-printing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T17:00:52.351302200Z",
     "start_time": "2025-02-18T17:00:52.199887900Z"
    }
   },
   "id": "26651b0308f8fa80",
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous avons dans un premier temps, installé **ollama** sur notre machine (en local) et avons forcé l'utilisation sur le **GPU** grâce au **panneau de configuration NVIDIA**.\n",
    "Nous devons vérifier qu'il tourne pour pouvoir faire des appels dessus via notre code python."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73adaec07225b4fb"
  },
  {
   "id": "9b4dd77a0f78f1fe",
   "cell_type": "code",
   "source": [
    "from ollama_evaluate import check_if_running, generate_model_scores\n",
    "\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ],
   "metadata": {
    "collapsed": false,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-17T19:49:22.727082Z",
     "iopub.status.idle": "2025-02-17T19:49:22.727376Z",
     "shell.execute_reply": "2025-02-17T19:49:22.727268Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-18T20:01:47.242369200Z",
     "start_time": "2025-02-18T20:01:47.068186400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maintenant que tout cela est fait, nous devons faire des appels sur ollama pour toutes les lignes de notre fichier. Nous obtiendrons un **score** sur une échelle de **0 à 100** avec 100 une réponse parfaite. Ollama donnera une note qui n'est pas basée que sur la correspondance des mots, puisque par exemple, si nous avons la traduction attendue 'Il fait très chaud aujourd'hui.' et que notre modèle traduit 'Aujourd’hui, il fait une chaleur intense.' cela demeure correct."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f667b2e7ae6ce8d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 17563/17563 [13:58:32<00:00,  2.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 17563 of 17563\n",
      "Average score: 75.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'translate.json'\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "model = \"llama3\"\n",
    "scores = generate_model_scores(json_data, \"model_response\", model)\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-20T19:44:00.540567700Z",
     "start_time": "2025-02-20T05:45:28.067825Z"
    }
   },
   "id": "ad9715c14257b98f",
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous obtenons un **score moyen** de **75.9**. \n",
    "\n",
    "Nous pensons que cela reflète un **excellent fine-tuning** puisque nous partons d'un modèle (**gpt2 M**) qui n'est **pas très performant de base**. Il n'y a que 355M de paramètres, ce qui fait que notre LLM peut avoir du mal à comprendre et interpréter, surtout que gpt2 n'a **pas** été **entrainé** sur du **français**. Ce score reflète globalement une bonne, voire une très bonne traduction. \n",
    "\n",
    "De ce que nous avons vu (nous l'avons testé via notre session CLI), le modèle fait très souvent des très bonnes traductions, parfois des traductions compréhensibles, mais avec des petites erreurs (e.g. il traduit 'what time is it ?' en 'A quelle heure il est ?' Cela est suffisant pour que nous le comprenions, mais n'est pas totalement juste. Et parfois, il n'arrive pas à traduire, si on lui met un mot inconnu par exemple, comme le prénom hugo ou titouan, vu que sur notre fonction generate, nous avons mis un petit topk (5) et une petite température (0.2), le modèle répond avec des mots dont la probabilité est importante.\n",
    "\n",
    "\n",
    "Nous sommes globalement très satisfait de nos traductions réalisées via la LLM et nous vous invitons à regarder rapidement quelques lignes du fichier 'translate.json' pour voir les traductions sur notre ensemble de test.\n",
    "Nous pensons que notre fine-tuning est suffisamment performant et ne nécessite pas d'être refait. Nous pourrions cependant chercher des paramètres plus 'optimaux' pour notre fonction generate (qui est dans translate).\n",
    "\n",
    "Pour véritablement améliorer nos traductions, il faudrait surtout prendre un modèle de base plus performant."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc73eeacfa5baa8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 - *'Deployment & Final Presentation'*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1487e29d7c200407"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comme nous l'avons dit auparavant, nous avons wrappé notre modèle pour l'utiliser dans une **session CLI**. Tout cela est disponible dans le fichier **'translate.py'**. Pour démarrer la session, ouvrez un terminal, rejoignez le dossier où se trouve le fichier et entrez : *python translate.py*. Cela ouvrira une 'session' que vous pourrez quitter via l'entrée d'*exit* ou de *quit*. Pour traduire, il suffira d'entrer le texte, par exemple : i need a backpack. Vous verrez la traduction juste en dessous (voir ci-dessous)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9a068e4924c1e64"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![CLI image](CLI.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55dd1df324c9bcce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Une vidéo de présentation des performances de notre modèle via ce script CLI a été réalisée et vous a été transmise."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59bdd3335392e9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous n'avons pas eu de difficulté de compréhension pour ce projet, cependant, il a été compliqué de faire tourner tous nos codes puisqu'ils prenaient du temps à compiler (bien que cela soit normal pour des LLM), notamment pour ollama, le code a pris 14h à tourner et nous avons dû le faire tourner plusieurs fois pour obtenir un résultat (pc qui surchauffe, etc).\n",
    "\n",
    "Pour toutes autres questions n'hésitez pas à nous contacter sur nos mails étudiants ESIEE Paris."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea24745204a5a6d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merci pour votre lecture,\n",
    "Titouan et Hugo."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5415b4f6691dcc"
  }
 ]
}
